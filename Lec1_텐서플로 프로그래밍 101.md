## 파이썬 실행해보기
```python
print('Hello, world')
```

    Hello, world
    


```python

```


```python
def f(a,b):
    return a+b

```


```python
f(1,2)

```




    3




```python

```


```python

```

- - -

## 1.1 텐서와 그래프 실행
### 1.  텐서 플로를 사용하기위해 텐서플로 라이브러리를 임포트.
다운 받은 버젼은 2.x 버젼이기때문에 2.x 버젼에서 1.x의 문법을 사용하기 위한 코드이다.

```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

hello= tf.constant('hello tensorflow!')
print(hello)
```
 hello 변수값을 출력한 결과로, hello가 텐서플로의 텐서라는 자료형이고, 상수를 담고있다.(경고는 무시)

    WARNING:tensorflow:From c:\users\안지혜\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\compat\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
    Instructions for updating:
    non-resource variables are not supported in the long term
    Tensor("Const:0", shape=(), dtype=string)
    




> 텐서 : 다양한 수학식을 계산하기 위한 가장 기본적이고 중요한 자료형. 텐서의 자료형은 배열과 비슷하다.
> * 랭크 : 차원의 수를 나타내는 것으로 0이면 스칼라, 1이면 벡터, 2이면 행렬,3이상이면 n-Tensor 또는 n차원 텐서라고 부른다.
> * 셰이프 : 각 차원의 요소의 개수. 텐서의 구조를 설명해준다.


### 2. 텐서를 이용해 다양한 연산을 수행할 수 있다.(덧셈 예시)
   > 일반적으로는 42가 나올것을 예상하지만 프린트된 값은 텐서의 형태를 출력한다.
   >
   > 이유: 텐서 플로 프로그램의 구조가 1.그래프 생성  2.그래프 실행 이 두가지로 분리 되어 있기 때문. 
   > 현재 단계는 그래프 생성 단계.
```python
a=tf.constant(10)
b=tf.constant(32)
c=tf.add(a,b)
print(c)
```

    Tensor("Add:0", shape=(), dtype=int32)
    
> ### 자연 실행 - 함수형 프로그램이에 많이 사용.      
> 1. 그래프 생성 : 텐서와 텐서의 연산들을 먼저 정의 하여 그래프를 만든다.
> 2. 그래프 실행 : 필요할때 연산을 실행하는 코드를 넣어 '원하는 시점'에 실제 연산이 수행하도록 한다.     





### 3. 그래프의 실행은 Session안에서 이뤄져야한다.(Session 객체와 run 메서드를 이용한다.)
```python
sess=tf.Session()
print(sess.run(hello))
print(sess.run([a,b,c]))
```

결과

    b'hello tensorflow!'
    [10, 32, 42]
    
   

- - -
## 1.2 플레이스홀더와 변수

* 플레이스 홀더 : 그래프에 사용할 입력값을 나중에 지정 하기 위해 사용하는 매개 변수.
* 변수 : 그래프를 최적화하기위해 텐서플로(학습 함수들)가 학습한 결과를 갱신하기 위해 사용하는 변수. - 신경망의 성능을 결정한다.      
### 1.플레이스 홀더 선언
```python
X=tf.placeholder(tf.float32,[None,3]) // None은 크기가 정해지지 않았음을 의미.
print(X)
```
결과

    Tensor("Placeholder:0", shape=(?, 3), dtype=float32)
    
### 2.플레이스 홀더X에 넣을 자료를 정의
X의 텐서모양이 (?,3) 이므로 두번째 차원은 요소를 3개씩 가지고 있어야한다.(첫번째 차원의 개수는 상관x)

```python
x_data=[[1,2,3],[4,5,6]]
```

### 3.변수 생성후 할당
tf.random_normal함수를 이용해 정규 분호의 무작위 값으로 초기화를 한다.    
W=tf.Variable([0.1,0.1],[0.2,0.2],[0.3,0.3])과 같이 직접 넣어도 가능하다.

```python
W=tf.Variable(tf.random_normal([3,2]))
b=tf.Variable(tf.random_normal([2,1]))
```

### 4.행렬곱을 수행할때는 tf.matmul함수를 사용해야한다.
```python
expr=tf.marmul(x,W)+b
```


    ---------------------------------------------------------------------------

    AttributeError                            Traceback (most recent call last)

    <ipython-input-15-f852ff78dc8d> in <module>
    ----> 1 expr=tf.marmul(x,W)+b
    

    AttributeError: module 'tensorflow_core.compat.v1' has no attribute 'marmul'


### 5.그래프 실행
- tf.global_variables_initializer() : 앞에서 정의한 변수들을 초기화 하는 함수. 초기화하지 않으면 오류가 난다.          
- feed_dict={X:x_data} : 그래프를 실행할때 사용할 입력 값을 지정한다. 지정하지 않으면 오류가 난다.
```python
sess.run(tf.global_variables_initializer())

print("==x_data==")
print(x_data)
print("===W===")
print(sess.run(W))
print("===b===")
print(sess.run(b))
print("===expr===")
print(sess.run(expt,feed_dict={X:x_data}))
sess.close()
```

결과

    ==x_data==
    [[1, 2, 3], [4, 5, 6]]
    ===W===
    [[-1.5963199  -0.9463549 ]
     [-0.56757915  0.4881241 ]
     [-0.23096584 -1.7002412 ]]
    ===b===
    [[-0.486335 ]
     [ 1.0353105]]
    ===expr===
    [[ -3.9107108  -5.557165 ]
     [ -9.573659  -10.510936 ]]
    
- - -

## 3.3 선형 회귀 모델 구현하기

- 선형 회귀 : 주어진 X 와 Y 값을 가지고 서로 간의 관계를 파악하는 것이다. 주어진 값들로 관계 파악을 하면 새로운 X값이 주어졌을 때, y값을 쉽게 알 수 있게 된다.

즉, 어떤 입력에 대한 출력을 예측 하는것. 머신 러닝의 기본이다.

### 1.플레이스 홀더 설정
tf.placeholder(tf.float32,name="X") : 플레이스 홀더에 이름을 설정가능하다. 설정하지 않으면 이름이 자동적으로 부여된다.
```python
x_data=[1,2,3]
y_data=[1,2,3]

W=tf.Variable(tf.random_uniform([1],-1.0,1.0))
b=tf.Variable(tf.random_uniform([1],-1.0,1.0))

X=tf.placeholder(tf.float32,name="X")
Y=tf.placeholder(tf.float32,name="Y")
```

### 2.상관 관계 수식 설정
X 와 Y의 상관관계를 분석하기 위한 수식이다.
W : 가중치, b: 편향.
행렬 곱이 아니기 때문에 * 사용가능. But, 행렬곱일 때는 tf.matmul 명령어를 사용해야 한다.
```
hypothesis=W*X+b
```



### 3.손실 함수
- 손실함수 : 손실 값을 계산하는 함수이다.
- 손실 값 : 실제값과 모델로 예측한 값이 얼마나 차이가 나는가를 나타내는 값.
          손실값이 작을 수록 X 와 Y 값을 정확하게 예측할 수 있다.
- 비용 : 전제 데이터에 대해 손실 값을 구한 경우 
- 학습 : 변수들의 값을 다양하게 넣어 계산해 보면서 손실값을 최소화하는 W와 b의 값을 구하는 것.

손실 값으로는 예측값과 실제값의 거리를 가장 많이 사용한다.
```
cost=tf.reduce_mean(tf.square(hypothesis-Y))
```

### 4.경사하강법 최적화 함수를 사용하여 손실값을 최소화하는 연산 그래프 생성.

- 최적화 함수 : 가중치와 편향 값을 변경해가면서 손실값을 최소화하는 가장 최적화된 가중치와 편향 값을 찾아주는 함수이다.
- learning_rate : 최적화 함수의 매개변수(== 학습률). 학습을 얼마나 '급하게' 할 것인가를 설정하는 값.(값 크면:최적의 손실값을 찾지못함, 값이 작으면 : 학습 속도가 매우 느려짐.)

```
optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.1)
train_op=optimizer.minimize(cost)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    
    for step in range(100):
        cost_val=sess.run([train_op,cost],feed_dict={X:x_data,Y:y_data})
        
        print(step,cost_val,sess.run(W),sess.run(b))
        
    print("\n====test====")
    print("X:5,Y:",sess.run(hypothesis,feed_dict={X:5}))
    print("X:2.5 ,Y:",sess.run(hypothesis,feed_dict={X:2.5}))

    
print("X:5,Y:",sess.run(hypothesis,feed_dict={X:5}))
```

    0 [None, 0.33694065] [0.79804826] [0.52194566]
    1 [None, 0.04112364] [0.77775824] [0.4983372]
    2 [None, 0.03582779] [0.785849] [0.48756647]
    3 [None, 0.03408606] [0.7906967] [0.47571358]
    4 [None, 0.032466456] [0.795761] [0.4642922]
    5 [None, 0.03092427] [0.80066717] [0.45312935]
    6 [None, 0.029455371] [0.80545944] [0.44223663]
    7 [None, 0.02805619] [0.810136] [0.43160555]
    8 [None, 0.026723498] [0.8147002] [0.42123005]
    9 [None, 0.025454104] [0.8191547] [0.41110396]
    10 [None, 0.024245039] [0.82350206] [0.4012213]
    11 [None, 0.023093373] [0.82774496] [0.39157623]
    12 [None, 0.02199643] [0.8318858] [0.382163]
    13 [None, 0.020951582] [0.8359272] [0.37297606]
    14 [None, 0.01995637] [0.8398714] [0.36400998]
    15 [None, 0.019008424] [0.8437208] [0.35525945]
    16 [None, 0.018105512] [0.8474776] [0.34671926]
    17 [None, 0.0172455] [0.85114413] [0.33838436]
    18 [None, 0.016426297] [0.8547225] [0.33024985]
    19 [None, 0.015646042] [0.8582149] [0.3223109]
    20 [None, 0.01490284] [0.8616233] [0.31456274]
    21 [None, 0.014194959] [0.86494976] [0.30700088]
    22 [None, 0.013520676] [0.86819625] [0.29962078]
    23 [None, 0.012878436] [0.8713648] [0.29241812]
    24 [None, 0.01226671] [0.8744571] [0.28538862]
    25 [None, 0.011684023] [0.877475] [0.27852803]
    26 [None, 0.011129024] [0.8804205] [0.27183244]
    27 [None, 0.010600381] [0.88329506] [0.26529774]
    28 [None, 0.01009687] [0.8861006] [0.2589202]
    29 [None, 0.009617246] [0.88883865] [0.25269592]
    30 [None, 0.009160427] [0.8915109] [0.24662128]
    31 [None, 0.008725302] [0.8941189] [0.24069266]
    32 [None, 0.008310835] [0.8966642] [0.23490657]
    33 [None, 0.007916067] [0.8991483] [0.22925957]
    34 [None, 0.007540053] [0.9015727] [0.22374834]
    35 [None, 0.007181891] [0.90393883] [0.21836957]
    36 [None, 0.006840749] [0.9062481] [0.21312013]
    37 [None, 0.0065158047] [0.9085018] [0.20799686]
    38 [None, 0.0062063113] [0.9107014] [0.20299676]
    39 [None, 0.0059114923] [0.91284806] [0.19811685]
    40 [None, 0.005630698] [0.9149431] [0.19335426]
    41 [None, 0.0053632376] [0.91698784] [0.18870616]
    42 [None, 0.005108483] [0.9189834] [0.1841698]
    43 [None, 0.004865829] [0.920931] [0.17974247]
    44 [None, 0.0046346835] [0.9228317] [0.17542157]
    45 [None, 0.004414544] [0.92468685] [0.17120458]
    46 [None, 0.004204853] [0.9264973] [0.16708893]
    47 [None, 0.004005108] [0.92826426] [0.16307223]
    48 [None, 0.0038148686] [0.9299887] [0.15915208]
    49 [None, 0.00363366] [0.93167174] [0.15532619]
    50 [None, 0.003461058] [0.9333143] [0.15159225]
    51 [None, 0.003296654] [0.9349174] [0.14794807]
    52 [None, 0.0031400656] [0.93648195] [0.1443915]
    53 [None, 0.0029909064] [0.93800884] [0.14092042]
    54 [None, 0.0028488368] [0.9394991] [0.13753279]
    55 [None, 0.0027135166] [0.9409535] [0.1342266]
    56 [None, 0.0025846178] [0.9423729] [0.1309999]
    57 [None, 0.002461848] [0.94375825] [0.12785076]
    58 [None, 0.0023449094] [0.94511026] [0.1247773]
    59 [None, 0.0022335218] [0.9464298] [0.12177775]
    60 [None, 0.002127425] [0.94771755] [0.11885029]
    61 [None, 0.0020263821] [0.94897443] [0.11599322]
    62 [None, 0.0019301167] [0.950201] [0.11320481]
    63 [None, 0.0018384418] [0.9513982] [0.11048347]
    64 [None, 0.0017511122] [0.9525665] [0.10782749]
    65 [None, 0.0016679381] [0.9537068] [0.10523541]
    66 [None, 0.0015887045] [0.9548196] [0.1027056]
    67 [None, 0.0015132359] [0.95590574] [0.10023663]
    68 [None, 0.0014413557] [0.95696574] [0.09782702]
    69 [None, 0.0013728944] [0.95800024] [0.09547531]
    70 [None, 0.0013076795] [0.9590099] [0.09318016]
    71 [None, 0.0012455651] [0.95999527] [0.09094017]
    72 [None, 0.0011863968] [0.96095693] [0.08875402]
    73 [None, 0.001130049] [0.9618956] [0.08662046]
    74 [None, 0.0010763626] [0.96281147] [0.08453811]
    75 [None, 0.0010252366] [0.9637055] [0.08250588]
    76 [None, 0.0009765381] [0.96457803] [0.08052251]
    77 [None, 0.00093015464] [0.96542954] [0.0785868]
    78 [None, 0.00088596944] [0.96626055] [0.07669762]
    79 [None, 0.00084388256] [0.96707165] [0.07485387]
    80 [None, 0.00080379844] [0.9678632] [0.07305443]
    81 [None, 0.0007656182] [0.9686358] [0.07129826]
    82 [None, 0.0007292498] [0.96938974] [0.06958428]
    83 [None, 0.00069460954] [0.97012556] [0.06791152]
    84 [None, 0.0006616173] [0.97084373] [0.06627899]
    85 [None, 0.0006301918] [0.9715447] [0.0646857]
    86 [None, 0.0006002561] [0.97222877] [0.06313071]
    87 [None, 0.00057174376] [0.97289634] [0.06161307]
    88 [None, 0.0005445843] [0.9735479] [0.06013191]
    89 [None, 0.00051871623] [0.97418374] [0.05868638]
    90 [None, 0.00049407734] [0.9748044] [0.05727562]
    91 [None, 0.00047060873] [0.97541004] [0.05589874]
    92 [None, 0.00044825338] [0.97600114] [0.05455498]
    93 [None, 0.000426962] [0.9765781] [0.05324354]
    94 [None, 0.0004066804] [0.97714114] [0.05196358]
    95 [None, 0.00038736223] [0.97769064] [0.05071441]
    96 [None, 0.00036896253] [0.97822696] [0.04949527]
    97 [None, 0.00035143676] [0.97875035] [0.04830543]
    98 [None, 0.00033474047] [0.97926116] [0.04714419]
    99 [None, 0.00031884215] [0.97975975] [0.04601089]
    
    ====test====
    X:5,Y: [4.94481]
    X:2.5 ,Y: [2.4954104]
    


    ---------------------------------------------------------------------------

    RuntimeError                              Traceback (most recent call last)

    <ipython-input-34-086d55cd3de5> in <module>
         27 
         28 
    ---> 29 print("X:5,Y:",sess.run(hypothesis,feed_dict={X:5}))
    

    c:\users\안지혜\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
        954     try:
        955       result = self._run(None, fetches, feed_dict, options_ptr,
    --> 956                          run_metadata_ptr)
        957       if run_metadata:
        958         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)
    

    c:\users\안지혜\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
       1101     # Check session.
       1102     if self._closed:
    -> 1103       raise RuntimeError('Attempted to use a closed Session.')
       1104     if self.graph.version == 0:
       1105       raise RuntimeError('The Session graph is empty.  Add operations to the '
    

    RuntimeError: Attempted to use a closed Session.



```python

```
